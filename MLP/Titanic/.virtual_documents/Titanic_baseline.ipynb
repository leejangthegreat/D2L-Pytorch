get_ipython().run_line_magic("matplotlib", " inline")
import numpy as np
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l


train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')


print(train_data.shape, test_data.shape)


print(train_data.iloc[0:4, :])
print([c for c in train_data])
print([c for c in test_data])


all_features = pd.concat((train_data.iloc[:, 2:], test_data.iloc[:, 1:]))
# 保留除Survived标签与ID以外的特征


print(all_features.shape)
print(all_features.iloc[:4, :])


all_labels = train_data.iloc[:, 1]


# Extracting Numeric features
numeric_index = all_features.dtypes[all_features.dtypes != 'object'].index

# Feature Normalization: E = 0, S2 = 1
all_features[numeric_index] = all_features[numeric_index].apply(lambda x: (x - x.mean()) / x.std())


print(all_features.iloc[:4, :])


# Fill N/A with 0 if data is unbiased
all_features[numeric_index] = all_features[numeric_index].fillna(0)


# Set dummy features
all_features = pd.get_dummies(all_features, dummy_na=True)


print(all_features.shape)


# Extract Numpy Metrix from Pandas Table
n_train = train_data.shape[0]
train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)
test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)
train_labels = torch.tensor(all_labels.values.reshape(-1, 1), dtype=torch.int64)


train_labels[:10]


# Define Module components
loss = nn.CrossEntropyLoss(reduction='none')
dim_inputs = train_features.shape[1]  # The feature number of one sample
net = nn.Sequential(nn.Linear(dim_inputs, 2))


def init_weight(N):
    if isinstance(N, nn.Linear):
        nn.init.normal_(N.weight, std=0.01)


net.apply(init_weight)


def accuracy(y_hat, y):
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = (y_hat.type(y.dtype) == y)
    return float(cmp.type(y.dtype).sum())


def evaluate(net, data_iter):
    if isinstance(net, nn.Module):
        net.eval()
    metric = d2l.Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(d2l.accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]


def train(net, train_features, train_labels, test_features, test_labels,
         num_epoches, lr, weight_dacay, batch_size):
    train_loss, train_acc, test_acc = [], [], []
    train_iter = d2l.load_array((train_features, train_labels), batch_size)
    optim = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=weight_dacay)
    for epoch in range(num_epoches):
        metric = d2l.Accumulator(3)
        net.train()
        for X, y in train_iter:
            optim.zero_grad()
            y_hat = net(X)
            l = loss(y_hat, y.reshape(-1))
            l.sum().backward()
            optim.step()
            with torch.no_grad():
                metric.add(l.mean() * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            train_acc.append(metric[1] / metric[2])
            train_loss.append(metric[0] / metric[2])
        if test_labels is not None:
            test_iter = d2l.load_array((test_features, test_labels), batch_size)
            test_acc.append(evaluate(net, test_iter))
    return train_acc, test_acc, train_loss


def fold_slice(k, fold_idx, X, y):
    assert k > 1
    X_train, y_train, X_valid, y_valid = None, None, None, None
    fold_size = X.shape[0] // k
    for i in range(k):
        idx = slice(i * fold_size, (i + 1) * fold_size)
        X_part, y_part = X[idx, :], y[idx]
        if i == fold_idx:
            X_valid, y_valid = X_part, y_part
        elif X_train is None:
            X_train, y_train = X_part, y_part
        else:
            X_train = torch.concat([X_train, X_part], 0)
            y_train = torch.concat([y_train, y_part], 0)
    return X_train, y_train, X_valid, y_valid


def k_fold_train(net, features, labels, num_epoches, lr, weight_decay, batch_size, k):
    """Train k fold on training dataset"""
    train_acc_sum, valid_acc_sum = 0, 0
    for fold in range(k):
        train_features, train_labels, test_features, test_labels = fold_slice(k, fold, features, labels)
        train_acc, test_acc, train_loss = train(net, train_features, train_labels, 
                                                test_features, test_labels, num_epoches, 
                                                lr, weight_decay, batch_size)
        train_acc_sum += train_acc[-1]
        valid_acc_sum += test_acc[-1]
        print(f"第{fold + 1}折：训练精度 {float(train_acc[-1]):f}, 训练损失 {float(train_loss[-1]):f}, 测试精度 {float(test_acc[-1]):f}")
    return train_acc_sum / k, valid_acc_sum / k


k = 5
num_epoches = 100
lr = 0.1
weight_decay = 2
batch_size = 32
train_acc, valid_acc = k_fold_train(net, train_features, train_labels, num_epoches, lr, weight_decay, batch_size, k)
print(f"{k} 折交叉验证，平均训练精度 {float(train_acc):f}，平均验证精度 {float(valid_acc):f}")



